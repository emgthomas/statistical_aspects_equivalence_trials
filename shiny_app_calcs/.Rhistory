only_replies <- as.numeric(names(only_replies)[only_replies==1]) # if fraction is one, that author only wrote replies in this journal
cases.only_replies[which_cases & cases$auth_id %in% only_replies] <- T
if((i %% 100)==0) print(i)
}
cat("\n\nDone.\n\n")
# put cases and controls back together
cases$only_replies <- cases.only_replies
controls$only_replies <- NA
authors4 <- rbind(cases[cases.keep,],controls[controls.keep,])
authors4$replies <- NULL
publications$replies <- NULL
cat("\n\n------- Flow chart Step 5 (Excluded)------- \n\n")
cat("Number of controls excluded = ",sum(1-authors2$case) - sum(1-authors4$case))
# Keep only top 10 controls
authors5 <- authors4 %>%
group_by(pub_id) %>%
mutate(match_score_rank = frank(-Match_Score))
authors5 <- authors5[order(authors5$pub_id,authors5$Match_Score,decreasing=T),]
# View(authors5[,c("pub_id","case","Match_Score","match_score_rank")])
authors5 <- authors5[authors5$match_score_rank <= 11, ] # keep 11 because the top match is always the case themself
cat("\n\n------- Flow chart Step 6 (Included)------- \n\n")
cat("Number of controls included after keeping only top 10 = ",sum(1-authors5$case))
#cat("Number of unique control authors included after keeping only top 10 = ",length(unique(authors5$auth_id[authors5$case==0])))
# Include only matched sets that have a case and at least one control
# i.e., remove invalid matched sets
checkFun <- function(status){
if(length(status)==1) return(FALSE)
if(sum(status) == 1 & length(status) > 1){
return(TRUE)
} else {
return(FALSE)
}
}
authors5$pub_id <- factor(authors5$pub_id,levels=unique(authors5$pub_id))
included.pubs <- tapply(authors5$case,authors5$pub_id,FUN=checkFun,simplify = T)
included.pubs <- data.frame(pub_id=names(included.pubs),include=included.pubs)
authors6 <- merge(authors5,included.pubs,by="pub_id",all.x=T)
authors6 <- authors6[authors6$include,]
authors6$include <- NULL
cat("\n\n------- Flow chart Step 7 (Included)------- \n\n")
cat("Number of matched sets = ",sum(authors6$case))
# For remaining controls, categorize match score into deciles
authors6$match_quantile <- numeric(length=nrow(authors6)) + NA
# find quantiles of match score for controls
authors6$match_quantile[authors6$case==0] <- quantcut(authors6$Match_Score[authors6$case==0],q=10)
# set "quantile" to 11 for cases, so they will always be included
authors6$match_quantile[authors6$case==1] <- 11
### put this together in final dataset
# "icc" stands for "intra-citing commentary"
icc_df_all <- authors6
# Remove authors with no gender designation
icc_df <- icc_df_all[icc_df_all$Gender != "unknown",]
icc_df$Publication <- factor(icc_df$pub_id,levels=unique(icc_df$pub_id))
included.pubs <- tapply(icc_df$case,icc_df$pub_id,FUN=checkFun,simplify = T)
included.pubs <- data.frame(pub_id=names(included.pubs),include=included.pubs)
icc_df <- merge(icc_df,included.pubs,by="pub_id",all.x=T)
icc_df <- icc_df[icc_df$include,]
icc_df$include <- NULL
cat("\n\n------- Flow chart Step 7 (Excluded)------- \n\n")
cat("Matched sets excluded due to missing gender = ",sum(authors6$case) - sum(icc_df$case))
cat("\n\n------- Flow chart Step 8 (Included)------- \n\n")
cat("Matched sets with complete gender information = ",sum(icc_df$case))
cat("\n\n------- Flow chart Step 9 (Included)------- \n\n")
cat("Number of journals in one-stage meta-analysis = ",length(unique(icc_df$pub_sourceid)))
###### Identify journals for which we can get a valid journal-level OR estimate ########
min_num_matched_sets <- 2
check_genders <- function(gender) length(unique(gender))==2
check_matched_sets <- function(idx,publications,gender,case,min_num_matched_sets){
publications <- publications[idx]
gender <- gender[idx]
case <- case[idx]
# which matched sets are not all the same gender?
out1 <- tapply(gender,as.character(publications),check_genders)
# after excluding matched sets that are all the same gender, there any "zero cells"?
out2 <- sum(table(gender[publications %in% names(out1)[out1]],case[publications %in% names(out1)[out1]])==0)
return(sum(out1)>=min_num_matched_sets & out2==0)
}
icc_df$Publication <- factor(icc_df$pub_id,levels=unique(icc_df$pub_id))
icc_df$pub_sourceid <- factor(icc_df$pub_sourceid,levels=unique(icc_df$pub_sourceid))
icc_df$Gender <- factor(icc_df$Gender,unique(icc_df$Gender))
included.journals <- tapply(1:nrow(icc_df),
icc_df$pub_sourceid,
check_matched_sets,
publications=icc_df$pub_id,
gender=icc_df$Gender,
case=icc_df$case,min_num_matched_sets=min_num_matched_sets)
included.journals <- data.frame(pub_sourceid=names(included.journals),include.journal=included.journals)
icc_df <- merge(icc_df,included.journals,by="pub_sourceid")
cat("\n\n------- Flow chart Step 9 (Excluded)------- \n\n")
cat("Journals with insufficient data for journal-specific estimate = ",length(included.journals$include.journal) - sum(included.journals$include.journal))
cat("\n\n------- Flow chart Step 10 (Included)------- \n\n")
cat("Journals with enough data for journal-specific estimate = ",sum(included.journals$include.journal))
cat("\n\nNumber of matched sets in these journals = ",sum(icc_df$case[icc_df$include.journal]))
# Create factor variables
icc_df$Gender <- factor(icc_df$Gender, levels=c("male","female"))
icc_df$pub_id <- factor(icc_df$pub_id, levels=unique(icc_df$pub_id))
icc_df$auth_id <- factor(icc_df$auth_id,levels=unique(icc_df$auth_id))
icc_df_all$Gender <- factor(icc_df_all$Gender, levels=c("male","female","unknown"))
icc_df_all$pub_id <- factor(icc_df_all$pub_id, levels=unique(icc_df_all$pub_id))
icc_df_all$auth_id <- factor(icc_df_all$auth_id,levels=unique(icc_df_all$auth_id))
# Measures of author seniority
icc_df$years_in_scopus <- 2019 - icc_df$First_Year_in_Scopus
icc_df_all$years_in_scopus <- 2019 - icc_df_all$First_Year_in_Scopus
icc_df$years_in_scopus_quintile <- as.numeric(quantcut(icc_df$years_in_scopus,q=5))
icc_df$h_index_quintile <- as.numeric(quantcut(icc_df$H_Index,q=5))
icc_df$n_pubs_quintile <- as.numeric(quantcut(icc_df$Total_Publications_In_Scopus,q=5))
#### Compute percentiles for seniority measures ####
# icc_df
YiS <- icc_df[,c("years_in_scopus","auth_id")]
YiS <- YiS[!duplicated(YiS),]
YiS$years_in_scopus_ptile <- percent_rank(YiS$years_in_scopus)/0.1
icc_df <- merge(icc_df,YiS[,c("auth_id","years_in_scopus_ptile")],by="auth_id",all.x=T,all.y=F)
h_index <- icc_df[,c("H_Index","auth_id")]
h_index <- h_index[!duplicated(h_index),]
h_index$h_index_ptile <- percent_rank(h_index$H_Index)/0.1
icc_df <- merge(icc_df,h_index[,c("auth_id","h_index_ptile")],by="auth_id",all.x=T,all.y=F)
n_pubs <- icc_df[,c("Total_Publications_In_Scopus","auth_id")]
n_pubs <- n_pubs[!duplicated(n_pubs),]
n_pubs$n_pubs_ptile <- percent_rank(n_pubs$Total_Publications_In_Scopus)/0.1
icc_df <- merge(icc_df,n_pubs[,c("auth_id","n_pubs_ptile")],by="auth_id",all.x=T,all.y=F)
# icc_df_all
YiS <- icc_df_all[,c("years_in_scopus","auth_id")]
YiS <- YiS[!duplicated(YiS),]
YiS$years_in_scopus_ptile <- percent_rank(YiS$years_in_scopus)/0.1
icc_df_all <- merge(icc_df_all,YiS[,c("auth_id","years_in_scopus_ptile")],by="auth_id",all.x=T,all.y=F)
h_index <- icc_df_all[,c("H_Index","auth_id")]
h_index <- h_index[!duplicated(h_index),]
h_index$h_index_ptile <- percent_rank(h_index$H_Index)/0.1
icc_df_all <- merge(icc_df_all,h_index[,c("auth_id","h_index_ptile")],by="auth_id",all.x=T,all.y=F)
n_pubs <- icc_df_all[,c("Total_Publications_In_Scopus","auth_id")]
n_pubs <- n_pubs[!duplicated(n_pubs),]
n_pubs$n_pubs_ptile <- percent_rank(n_pubs$Total_Publications_In_Scopus)/0.1
icc_df_all <- merge(icc_df_all,n_pubs[,c("auth_id","n_pubs_ptile")],by="auth_id",all.x=T,all.y=F)
#### Merge in Asian origin data ####
authors_asian <- readRDS("./data/authors_asian.rds")
# merge into authors dataset
icc_df_all <- merge(icc_df_all,authors_asian,by="auth_id",all.x=T,all.y=F)
#### Merge in journal topics ####
# Read in journal topics
journal_topics <- read.csv(file="./data/All_Journals_ASJC.csv")
# Keep only medical or multidisciplinary topics (some journals have extra ASJC codes outside our range of interest)
journal_topics <- journal_topics[(journal_topics$AJSC_Codes < 2800 & journal_topics$AJSC_Codes >= 2700) |
journal_topics$AJSC_Codes==1000,]
journal_names <- journal_topics[!duplicated(journal_topics$pub_sourceid),c("pub_sourceid","sourcetitle")]
topic_names <- read.csv(file="./data/ASJC Codes with levels.csv",
sep=";") # cloned from github.com/plreyes/Scopus.git
journal_topics <- merge(x=journal_topics,y=topic_names,
by.x="AJSC_Codes",by.y="Code",
all.x=T)
# Create dataframe of topics by journal
journal_topics_low <- dcast(journal_topics,pub_sourceid ~ Low,fun.aggregate = length, value.var="Low")
journal_topics_low <- merge(journal_topics_low,journal_names,by="pub_sourceid")
journal_topics_low <- journal_topics_low[,c(ncol(journal_topics_low),1:(ncol(journal_topics_low)-1))]
############## Save data #############
saveRDS(icc_df_all,file="./data/processed_data_all.rds")
saveRDS(icc_df,file="./data/processed_data_no_missing.rds")
saveRDS(journal_topics_low,file="./data/journal_topics.rds")
#######################################################################
sink()
#######################################################################
require(BoomSpikeSlab)
require(circlize)
require(collapsibleTree)
require(data.table)
require(doParallel)
require(ggplot2)
require(glue)
require(icd)
require(igraph)
require(Matrix)
require(mclust)
require(plotly)
require(RColorBrewer)
require(reshape2)
require(xtable)
.packages()
(.packages())
sessionInfo()
0.878*49
0.74*50
0.64*50
0.614*57
0.574*47
0.563*48
0.158*57
0.14*57
0.46*50
31.8*44
.318*44
.228*57
0.333 + 0.509
0.246 + 0.351
0.737*57
0.667*57
0.736*57
0.842*57
57*0.491
0.679*57
0.491*57
0.65*57
0.491*57
0.421*57
0.596*57
0.474*57
0.281&57
0.281*57
18+0+2
18+9+2
31+6.9
0.7*40
(42.1+5.3)*57/100
(42.1+5.3)
0.675*40
0.86*57
0.79*57
0.754*57
0.684*57
0.491*57
0.381*57
0.388*57
32+10+2
(0.561 + 0.175 + 0.035)
22+17+9
0.386+0.298+0.158
20+14+6
(0.351 + 0.246 + 0.105)
17.5+3.5
15.8+7.0
5.3+10.5
29.8+15.8
57/71
31/70
32/70
(22+3+2)/(22+3+2+29)
(18+4+2+2)/(18+4+2+2+22)
(18+4+2+2)/(18+4+2+2+22)
(18+4+2+2)
19/70
8/60
8/57
18/(18+36)
0.298*57
57/70
100-29.8
0.702*57
(15+19+14)
(0.842*59)
(0.842*57)
(0.842*59)
(0.840*59)
(0.840*57)
21.1 + 24.6 + 8.8 + 3.5
12 + 14 + 5 + 2
require(metafor)
??metafor
?rma.mv
?prop.test
# 2.2 ---------------------------------------------------------------
require(lobstr)
x <- c(1,2,3)
y <- x
obj_addr(x)
obj_addr(y)
?Reserved
obj_addr(mean)
obj_addr(get("mean"))
obj_addr(evaluq(mean))
obj_addr(evalq(mean))
obj_addr(match.fun("mean"))
blah <- "mean"
obj_addr(get(blah))
obj_addr(match.fun(blah))
?read.csv
?make.names
blah <- read.csv("`1blah'\n,1/n2/n3/n")
?read.csv
blah <- read.csv(file("`1blah'\n,1/n2/n3/n"))
?make.names
make.names("1blah")
make.names(".blah")
make.names("_blah")
make.names(".2blah")
make.names(".123e1")
.123e1
make.names(".123")
make.names(".123f")
make.names(".f123")
x <- 1:3
obj_addr(x)
x <- 1:3
obj_addr(x)
x[2] <- 5
obj_addr(x)
tracemem(x)
y <- x
y[[3]] <- 5L
obj_addr(x)
obj_addr(y)
untracemem(x)
# 2.2 ---------------------------------------------------------------
require(lobstr)
x <- c(1,2,3)
tracemem(x)
y <- x
obj_addr(x)
obj_addr(y)
y[[3]] <- 5L
lobstr::untracemem(x)
?tracememe
?tracemem
untracemem(x)
# 2.2 ---------------------------------------------------------------
require(lobstr)
x <- c(1,2,3)
tracemem(x)
y <- x
lobstr::obj_addr(x)
lobstr::obj_addr(y)
y[[3]] <- 5L
lobstr::obj_addr(x)
lobstr::obj_addr(y)
untracemem(x)
# 2.3 ---------------------------------------------------------------
f <- function(a) {
a
}
x <- c(1, 2, 3)
tracemem(x)
z <- f(x)
untracemem(x)
# 2.3 ---------------------------------------------------------------
f <- function(a) {
a
}
x <- c(1, 2, 3)
tracemem(x)
z <- f(x)
lobstr::obj_addr(x)
lobstr::obj_addr(z)
untracemem(x)
# lists
l1 <- list(1, 2, 3)
lobstr::obj_addr(l1)
lobstr::obj_addr(l1[[1]])
lobstr::obj_addr(l1[[2]])
lobstr::obj_addr(l1[[3]])
lobstr::obj_addr(l1[[3]])
a <- l1[[3]]
lobstr::obj_addr(a)
x
lobstr::obj_addr(x[1])
lobstr::obj_addr(x[2])
lobstr::obj_addr(x[3])
z <- x[2]
lobstr::obj_addr(z)
lobstr::obj_addr(x[2])
a
l1[[3]]
lobstr::ref(l1,l2)
l2 <- l1
l2[[3]] <- 4
lobstr::ref(l1,l2)
d1 <- data.frame(x = c(1, 5, 6), y = c(2, 4, 3))
d2 <- d1
d2[, 2] <- d2[, 2] * 2
lobstr::ref(d1,d2)
lobstr::ref(d1,d3)
d3 <- d1
d3[1, ] <- d3[1, ] * 3 # modifying a row
lobstr::ref(d1, d3)
lobstr::ref(x)
# characters
x <- c("a", "a", "abc", "d")
lobstr::ref(x)
lobstr::ref(x, character = TRUE)
x <- c(1L, 2L, 3L)
tracemem(x)
x[[3]] <- 4
x <- c(1L, 2L, 3L)
tracemem(x)
x[[3]] <- 4
untracemem(x)
x <- c(1, 2, 3)
tracemem(x)
x[[3]] <- 4L
untracemem(x)
a <- 1:10
b <- list(a, a)
obj_addr(a)
obj_addr(b)
ref(a,b)
c <- list(b, a, 1:10)
ref(a,c)
x <- list(1:10)
x[[2]] <- x
ref(x)
x
x <- list(1:10)
x
x[[2]] <- x
x
ref(x)
x <- list(1:10)
tracemem(x)
x[[2]] <- x # copy-on-modify
ref(x)
# 2.4 ------------------------------------------------------------------
object.size(x)
lobstr::obj_size(x)
?obj_size
# characters
x <- c("a", "a", "abc", "d")
lobstr::ref(x, character = TRUE)
# 2.4 ------------------------------------------------------------------
object.size(x) # wrong
lobstr::obj_size(x) # right - correctly accounts for shared references
x <- list(1:10)
x <- list(1:10)
tracemem(x)
x[[2]] <- x # copy-on-modify
untracemem(x)
ref(x)
# 2.4 ------------------------------------------------------------------
object.size(x) # wrong
lobstr::obj_size(x) # right - correctly accounts for shared references
# more on lists
x <- list(1:10)
tracemem(x)
x[[2]] <- x # copy-on-modify
untracemem(x)
ref(x)
# 2.4 ------------------------------------------------------------------
object.size(x) # wrong
lobstr::obj_size(x) # right - correctly accounts for shared references
x <- runif(1e6)
obj_size(x)
y <- list(x, x, x)
obj_size(y)
obj_size(x,y) # combined object size
R.Version()
R.version.string
R.version.string
install.packages("tidyverse")
options(warn = 1)
f()
f <- function() warning("I'm warning you!")
options(warn = 1)
f()
f <- function() {
cat("I have something to say.\n\n")
warning("I'm warning you!")
cat("I have something new to say.\n\n")
}
options(warn = 1)
f()
options(warn = 2)
f()
options(warn = 3)
f()
f <- function() {
cat("I have something to say.\n")
warning("I'm warning you!")
cat("I have something new to say.\n")
}
options(warn = 0)
f()
options(warn = 1)
f()
options(warn = 2)
f()
f <- function() {
cat("I have something to say.\n")
# warning("I'm warning you!")
rlang::warn("I'm warning you!")
cat("I have something new to say.\n")
}
options(warn = 0)
f()
options(warn = 1)
f()
options(warn = 2)
f()
f <- function() message("I'm just the messenger!")
f()
f <- function(){
cat("I have something to say.\n")
message("I'm just the messenger!")
cat("I have something new to say.\n")
}
f()
# Exercise 8.2
file.remove2 <- function(file) {
if(!file.exists(file)) stop("File does not exist!")
file.remove(file)
}
file.remove("no.txt")
file.exists("no.text")
file.remove("blah.txt")
options(warn = 0)
# Exercise 8.2
file.remove2 <- function(file) {
if(!file.exists(file)) stop("File does not exist!")
file.remove(file)
}
file.remove2("non_existant_file.txt")
?message
log("x")
install.packages(c("plotly", "shinyWidgets"))
install.packages("plotly")
shiny::runApp('Documents/PhD_Papers/Gender_bias/R_code/gender_and_invited_commentaries/github/shiny_app')
runApp('Documents/PhD_Papers/Gender_bias/R_code/gender_and_invited_commentaries/github/shiny_app')
shiny::runApp('Documents/PhD_Papers/Non-inferiority trials/Code/statistical_aspects_equivalence_trials/shiny_app_calcs')
